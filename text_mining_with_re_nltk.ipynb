{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ggEXrnI8_804"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python"
      ],
      "metadata": {
        "id": "1r7vhwTw9pVv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Text Mining with re and NLTK\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "sp4tsx-f9ssN",
        "outputId": "846259ea-9116-4e51-be80-8c752d0ff8db"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Text Mining with re and NLTK'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lesson 12 Assignment"
      ],
      "metadata": {
        "id": "__v5yPpP90vj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "import re\n",
        "import nltk"
      ],
      "metadata": {
        "id": "rYexgvOzfgww"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "cer1_SnejAta"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Use the following codes to load the assignment12.txt Download assignment12.txt which contains file names. How many file names are in it? (10 points)"
      ],
      "metadata": {
        "id": "i1aNyID8jVGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"Assignment_12.txt\" , 'r')\n",
        "text1 = file.read()\n",
        "file.close()"
      ],
      "metadata": {
        "id": "ycZ1y4A5u6I0"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "bobqSHHzf7dr",
        "outputId": "5173300e-d069-4b3f-e428-fc696b09e066"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'arxiv_annotate10_7_1.txt   arxiv_annotate10_7_2.txt   arxiv_annotate10_7_3.txt   arxiv_annotate1_13_1.txt   arxiv_annotate1_13_2.txt   arxiv_annotate1_13_3.txt   arxiv_annotate2_66_1.txt   arxiv_annotate2_66_2.txt   arxiv_annotate2_66_3.txt   arxiv_annotate3_80_1.txt   arxiv_annotate3_80_2.txt   arxiv_annotate3_80_3.txt   arxiv_annotate4_168_1.txt   arxiv_annotate4_168_2.txt   arxiv_annotate4_168_3.txt   arxiv_annotate5_240_1.txt   arxiv_annotate5_240_2.txt   arxiv_annotate5_240_3.txt   arxiv_annotate6_52_1.txt   arxiv_annotate6_52_2.txt   arxiv_annotate6_52_3.txt   arxiv_annotate7_268_1.txt   arxiv_annotate7_268_2.txt   arxiv_annotate7_268_3.txt   arxiv_annotate8_81_1.txt   arxiv_annotate8_81_2.txt   arxiv_annotate8_81_3.txt   arxiv_annotate9_279_1.txt   arxiv_annotate9_279_2.txt   arxiv_annotate9_279_3.txt   jdm_annotate10_210_1.txt   jdm_annotate10_210_2.txt   jdm_annotate10_210_3.txt   jdm_annotate1_103_1.txt   jdm_annotate1_103_2.txt   jdm_annotate1_103_3.txt   jdm_annotate2_107_1.txt   jdm_annotate2_107_2.txt   jdm_annotate2_107_3.txt   jdm_ann^otate3_120_1.txt   jdm_annotate3_120_2.txt   jdm_annotate3_120_3.txt   jdm_annotate4_220_1.txt   jdm_annotate4_220_2.txt   jdm_annotate4_220_3.txt   jdm_annotate5_228_1.txt   jdm_annotate5_228_2.txt   jdm_annotate5_228_3.txt   jdm_annotate6_32_1.txt   jdm_anno&tate6_32_2.txt   jdm_annotate6_32_3.txt   jdm_annotate7_265_1.txt   jdm_annotate7_265_2.txt   jdm_annotate7_265_3.txt   jdm_annotate8_177_1.txt   jdm_annotat#e8_177_2.txt   jdm_annotate8_177_3.txt   jdm_annotate9_45_1.txt   jdm_annotate9_45_2.txt   jdm_annotate9_45_3.txt   plos_annotate10_1140_1.txt   plos_annotate10_1140_2.txt   plos_annotate10_1140_3.txt   plos_annotate1_6_1.txt   plos_annotat*e1_6_2.txt   plos_annotate1_6_3.txt   plos_annotate2_336_1.txt   plos_annotate2_336_2.txt   plos_annotate2_336_3.txt   plos_annotate3_798_1.txt   plos_annotate3_798_2.txt   plos_annotate3_798_3.txt   plos_annotate4_1052_1.txt   plos_annotate4_1052_2.txt   plos_annotate4_1052_3.txt   plos_annotate5_1375_1.txt   plos_annotate5_1375_2.txt   plos_anno%tate5_1375_3.txt   plos_annotate6_1032_1.txt   plos_annotate6_1032_2.txt   plos_annotate6_1032_3.txt   plos_annotate7_1233_1.txt   plos_annot@ate7_1233_2.txt   plos_annotate7_1233_3.txt   plos_annotate8_123_1.txt   plos_annotate8_123_2.txt   plos_annotate8_123_3.txt   plos_annotate9_1187_1.txt   plos_annotate9_1187_2.txt   plos_annotate9_1187_3.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The text file consists of various file names. Each file name is separated by multiple white spaces. Since we know that each file name is seperated by white space, we can use `split` and `len` to easily get the number of file names present."
      ],
      "metadata": {
        "id": "wbgO14iQgLNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "names = re.split('\\s+', text1)\n",
        "len(names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qy2CQa1fikHz",
        "outputId": "ecf957fc-fedf-4138-f848-6ed1b2c14ad7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "90"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 90 filenames present in the text."
      ],
      "metadata": {
        "id": "EL-ShRD1jd3b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Identify the pattern of the file namesï¼Œ and find out how many file names match the pattern. (20 points)\n"
      ],
      "metadata": {
        "id": "hlzgwxNV_-qt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "uNVj1fPKj3NI",
        "outputId": "5845405c-5173-4197-86ed-f71316efd409"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'arxiv_annotate10_7_1.txt   arxiv_annotate10_7_2.txt   arxiv_annotate10_7_3.txt   arxiv_annotate1_13_1.txt   arxiv_annotate1_13_2.txt   arxiv_annotate1_13_3.txt   arxiv_annotate2_66_1.txt   arxiv_annotate2_66_2.txt   arxiv_annotate2_66_3.txt   arxiv_annotate3_80_1.txt   arxiv_annotate3_80_2.txt   arxiv_annotate3_80_3.txt   arxiv_annotate4_168_1.txt   arxiv_annotate4_168_2.txt   arxiv_annotate4_168_3.txt   arxiv_annotate5_240_1.txt   arxiv_annotate5_240_2.txt   arxiv_annotate5_240_3.txt   arxiv_annotate6_52_1.txt   arxiv_annotate6_52_2.txt   arxiv_annotate6_52_3.txt   arxiv_annotate7_268_1.txt   arxiv_annotate7_268_2.txt   arxiv_annotate7_268_3.txt   arxiv_annotate8_81_1.txt   arxiv_annotate8_81_2.txt   arxiv_annotate8_81_3.txt   arxiv_annotate9_279_1.txt   arxiv_annotate9_279_2.txt   arxiv_annotate9_279_3.txt   jdm_annotate10_210_1.txt   jdm_annotate10_210_2.txt   jdm_annotate10_210_3.txt   jdm_annotate1_103_1.txt   jdm_annotate1_103_2.txt   jdm_annotate1_103_3.txt   jdm_annotate2_107_1.txt   jdm_annotate2_107_2.txt   jdm_annotate2_107_3.txt   jdm_ann^otate3_120_1.txt   jdm_annotate3_120_2.txt   jdm_annotate3_120_3.txt   jdm_annotate4_220_1.txt   jdm_annotate4_220_2.txt   jdm_annotate4_220_3.txt   jdm_annotate5_228_1.txt   jdm_annotate5_228_2.txt   jdm_annotate5_228_3.txt   jdm_annotate6_32_1.txt   jdm_anno&tate6_32_2.txt   jdm_annotate6_32_3.txt   jdm_annotate7_265_1.txt   jdm_annotate7_265_2.txt   jdm_annotate7_265_3.txt   jdm_annotate8_177_1.txt   jdm_annotat#e8_177_2.txt   jdm_annotate8_177_3.txt   jdm_annotate9_45_1.txt   jdm_annotate9_45_2.txt   jdm_annotate9_45_3.txt   plos_annotate10_1140_1.txt   plos_annotate10_1140_2.txt   plos_annotate10_1140_3.txt   plos_annotate1_6_1.txt   plos_annotat*e1_6_2.txt   plos_annotate1_6_3.txt   plos_annotate2_336_1.txt   plos_annotate2_336_2.txt   plos_annotate2_336_3.txt   plos_annotate3_798_1.txt   plos_annotate3_798_2.txt   plos_annotate3_798_3.txt   plos_annotate4_1052_1.txt   plos_annotate4_1052_2.txt   plos_annotate4_1052_3.txt   plos_annotate5_1375_1.txt   plos_annotate5_1375_2.txt   plos_anno%tate5_1375_3.txt   plos_annotate6_1032_1.txt   plos_annotate6_1032_2.txt   plos_annotate6_1032_3.txt   plos_annotate7_1233_1.txt   plos_annot@ate7_1233_2.txt   plos_annotate7_1233_3.txt   plos_annotate8_123_1.txt   plos_annotate8_123_2.txt   plos_annotate8_123_3.txt   plos_annotate9_1187_1.txt   plos_annotate9_1187_2.txt   plos_annotate9_1187_3.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's go through each of the filenames from beginning to end and break down their structure. Firstly, we can see each file name begins with either \"arxiv\", \"jdm\", or \"plos\". The regular expression \"[A-Z]{3,5}\" will identify these substrings. Those characters are followed by \"\\_annotate\", which makes our regular expression \"[A-Z]{3,5}\\_annotate\". The final part of the filename consists of combinations of numbers, underscores and the .txt ending. The regular expression\n",
        "\"[0-9]+\\_[0-9]+\\_[0-9].txt\" should complete the pattern. So, the final regex pattern would be \"[A-Z]{3,5}\\_annotate[0-9]+\\_[0-9]+_[0-9]\". We will use this regex pattern to match all file names that follow the pattern, then find the length of the matches."
      ],
      "metadata": {
        "id": "URQq22qDj9Oe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = re.compile('[A-Z]{3,5}_annotate[0-9]+_[0-9]+_[0-9].txt', flags=re.IGNORECASE)\n",
        "matches = pattern.findall(text1)\n",
        "len(matches)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDIVHL76n02J",
        "outputId": "4b81ff67-8222-4a20-8b3b-a41b19530911"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "84"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There appears to be 84 file names that match the pattern we drafted."
      ],
      "metadata": {
        "id": "h3XuDiqnoLPi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Find out file names that don't match with the pattern you designed. (20 points)"
      ],
      "metadata": {
        "id": "2FAzr7AXABtm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "non_matches = [name for name in names if (name not in matches)]\n",
        "non_matches"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQ_8utc4oofA",
        "outputId": "5895be2b-32de-4113-d405-a9531a79ed61"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['jdm_ann^otate3_120_1.txt',\n",
              " 'jdm_anno&tate6_32_2.txt',\n",
              " 'jdm_annotat#e8_177_2.txt',\n",
              " 'plos_annotat*e1_6_2.txt',\n",
              " 'plos_anno%tate5_1375_3.txt',\n",
              " 'plos_annot@ate7_1233_2.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above output show the remaining 6 file names that do not match the regex pattern we created. We can notice that all 6 file names include a special character in the word annotate."
      ],
      "metadata": {
        "id": "4vDMT8y0QCqV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Use the following codes to read the text from arxiv_annotate1_13_1.txt. Normalize the words and find out their counts."
      ],
      "metadata": {
        "id": "Z521GcaQAF47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = open('arxiv_annotate1_13_1.txt', 'r')\n",
        "text2 = file.read()\n",
        "file.close()"
      ],
      "metadata": {
        "id": "5AmtN51OQnqG"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "WJmqVTo7SOMX",
        "outputId": "2a99d06f-619c-4525-bd6c-17a4c1ec7882"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'### abstract ###\\nMISC\\talthough the internet as level topology has been extensively studied over the past few years  little is known about the details of the as taxonomy\\nMISC\\tan as  node  can represent a wide variety of organizations  e g   large isp  or small private business  university  with vastly different network characteristics  external connectivity patterns  network growth tendencies  and other properties that we can hardly neglect while working on veracious internet representations in simulation environments\\nAIMX\\tin this paper  we introduce a radically new approach based on machine learning techniques to map all the ases in the internet into a natural as taxonomy\\nOWNX\\twe successfully classify  NUMBER   NUMBER  percent  of ases with expected accuracy of  NUMBER   NUMBER  percent \\nOWNX\\twe release to the community the as level topology dataset augmented with   NUMBER   the as taxonomy information and  NUMBER   the set of as attributes we used to classify ases\\nOWNX\\twe believe that this dataset will serve as an invaluable addition to further understanding of the structure and evolution of the internet\\n### introduction ###\\nMISC\\tthe rapid expansion of the internet in the last two decades has produced a large scale system of thousands of diverse  independently managed networks that collectively provide global connectivity across a wide spectrum of geopolitical environments\\nMISC\\tfrom  NUMBER  to  NUMBER  the number of globally routable as identifiers has increased from less than  NUMBER   NUMBER  to more than  NUMBER   NUMBER   exerting significant pressure on interdomain routing as well as other functional and structural parts of the internet\\nMISC\\tthis impressive growth has resulted in a heterogenous and highly complex system that challenges accurate and realistic modeling of the internet infrastructure\\nMISC\\tin particular  the as level topology is an intermix of networks owned and operated by many different organizations  e g   backbone providers  regional providers  access providers  universities and private companies\\nMISC\\tstatistical information that faithfully characterizes different as types is on the critical path toward understanding the structure of the internet  as well as for modeling its topology and growth\\nMISC\\tin topology modeling  knowledge of as types is mandatory for augmenting synthetically constructed or measured as topologies with realistic intra as and inter as router level topologies\\nMISC\\tfor example  we expect the network of a dual homed university to be drastically different from that of a dual homed small company\\nMISC\\tthe university will likely contain dozens of internal routers  thousands of hosts  and many other network elements  switches  servers  firewalls \\nMISC\\ton the other hand  the small company will most probably have a single router and a simple network topology\\nMISC\\tsince there is such a diversity among different network types  we cannot accurately augment the as level topology with appropriate router level topologies if we cannot MISC\\tcharacterize the composing ases\\nMISC\\tmoreover  annotating the ases in the as topology with their types is a prerequisite for modeling the evolution of the internet  since different types of ases exhibit different growth patterns\\nMISC\\tfor example  internet service providers  isp  grow by attracting new customers and by engaging in business agreements with other isps\\nMISC\\ton the other hand  small companies that connect to the internet through one or few isps do not grow significantly over time\\nMISC\\tthus  categorizing different types of ases in the internet is necessary to identify network evolution patterns and develop accurate evolution models\\nMISC\\tan as taxonomy is also necessary for mapping ip addresses to different types of users\\nMISC\\tfor example  in traffic analysis studies its often required to distinguish between packets that come from home and business users\\nMISC\\tgiven an as taxonomy  its possible to realize this goal by checking the type of as that originates the prefix in which an ip address lies\\nAIMX\\tin this work  we introduce a radically new approach based on machine learning to construct a representative as taxonomy\\nOWNX\\twe develop an algorithm to classify ases based on empirically observed differences between as characteristics\\nOWNX\\twe use a large set of data from the internet routing registries  irr   CITATION  and from routeviews  CITATION  to identify intrinsic differences between ases of different types\\nAIMX\\tthen  we employ a novel machine learning technique to build a classification algorithm that exploits these differences to classify ases into six representative classes that reflect ases with different network properties and infrastructures\\nOWNX\\twe derive macroscopic statistics on the different types of ases in the internet and validate our results using a sample of  NUMBER  manually identified as types\\nOWNX\\tour validation demonstrates that our classification algorithm achieves high accuracy   NUMBER   NUMBER  percent  of the examined classifications were correct\\nOWNX\\tfinally  we make our results and our classifier publicly available to promote further research and understanding of the internet s structure and evolution\\nOWNX\\tin section  we start with a brief discussion of related work\\nOWNX\\tsection  describes the data we used  and in section  we specify the set of as classes we use in our experiments\\nOWNX\\tsection  introduces our classification approach and results\\nOWNX\\twe validate them in section  and conclude in section \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's begin by simply looking at the text to understand what we are working with. We can see there are many new lines and tabs present in the text. In between the newlines and tabs, there is either MISC, AIMX or OWNX. These words do not look like they provide any context to the excerpt. We will remove thes during our normalization process. We can also see there are section headings and #s used to format these headings. We will also remove these as formatting will not be necessary."
      ],
      "metadata": {
        "id": "5JjuRCe8TpZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "tokens = nltk.word_tokenize(text2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRBTCGLySzpk",
        "outputId": "2dd6fc9b-9a30-4dd0-e263-564ff6c4ed76"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we being normalizing, let's count the vocabulary of the text. We can do this by lowering the token we created above. Then, we will count the amount of unique words and words in general."
      ],
      "metadata": {
        "id": "6LWiHKrOXgkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lower_tokens = list(map(lambda x: x.lower(), tokens))"
      ],
      "metadata": {
        "id": "x6a79c1NVSYF"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Number of words: {len(lower_tokens)}\\n', f'Number of unique words: {len(set(lower_tokens))}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2xRlm0HYCmv",
        "outputId": "2318c068-0881-4ed0-ea87-0f2335b3d99d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words: 856\n",
            " Number of unique words: 333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! Now we know there about 856 words and over 1/3 of them are unique. Next, we will remove the words and characters mentioned above."
      ],
      "metadata": {
        "id": "P2OL38tmZIE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "remove = ['#', 'misc', 'introduction', 'abstract', 'ownx', 'aimx']\n",
        "clean_text = [x for x in lower_tokens if x not in remove]\n"
      ],
      "metadata": {
        "id": "yVmDY4_JU6AP"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have removed all unnecessary words and characters, we can use the word lemmatizer from the `nltk` library. This package will allow us to stem the words of their endings, but unlike the steming function, the word lemmatizer will only return words that are part of the dictionary. Let's perform the lemmatization then look at the differences in the words before and after lemmatizing."
      ],
      "metadata": {
        "id": "tGgBT9aI8RHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "lem = nltk.WordNetLemmatizer()\n",
        "lem_words = [lem.lemmatize(word) for word in clean_text]"
      ],
      "metadata": {
        "id": "oUK1vKVJiMT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print([x for x in lem_words if x not in clean_text])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RE9Mh2oGisy_",
        "outputId": "6c158ac6-3419-44b5-d7b3-11a049d94998"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ha', 'year', 'detail', 'organization', 'characteristic', 'pattern', 'tendency', 'property', 'representation', 'environment', 'attribute', 'decade', 'ha', 'thousand', 'environment', 'identifier', 'ha', 'le', 'part', 'ha', 'challenge', 'organization', 'provider', 'provider', 'provider', 'it', 'dozen', 'thousand', 'host', 'element', 'switch', 'server', 'firewall', 'pattern', 'provider', 'customer', 'agreement', 'pattern', 'model', 'user', 'study', 'it', 'packet', 'user', 'it', 'lie', 'difference', 'characteristic', 'registry', 'difference', 'exploit', 'difference', 'class', 'property', 'statistic', 'result', 'result', 'class', 'experiment', 'result']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print([x for x in clean_text if x not in lem_words])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjIxuZNmi9aX",
        "outputId": "a4d58c79-fa96-432d-8822-6a9bea2906ce"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['has', 'years', 'details', 'organizations', 'characteristics', 'patterns', 'tendencies', 'properties', 'representations', 'environments', 'techniques', 'ases', 'ases', 'attributes', 'ases', 'decades', 'has', 'thousands', 'networks', 'environments', 'identifiers', 'has', 'less', 'parts', 'has', 'challenges', 'networks', 'organizations', 'providers', 'providers', 'providers', 'universities', 'companies', 'types', 'its', 'types', 'topologies', 'topologies', 'dozens', 'routers', 'thousands', 'hosts', 'elements', 'switches', 'servers', 'firewalls', 'types', 'topologies', 'ases', 'ases', 'types', 'types', 'ases', 'patterns', 'providers', 'customers', 'agreements', 'companies', 'types', 'ases', 'patterns', 'models', 'addresses', 'types', 'users', 'studies', 'its', 'packets', 'users', 'its', 'lies', 'ases', 'differences', 'characteristics', 'registries', 'differences', 'ases', 'types', 'exploits', 'differences', 'ases', 'classes', 'ases', 'properties', 'infrastructures', 'statistics', 'types', 'ases', 'results', 'types', 'classifications', 'results', 'classes', 'experiments', 'results']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are a few key difference between the non-lemmatized and lemmatized words. Firstly, we can notice that plural words are stemmed to the singular version. For example, organizations become organization. The lemmatizer is also able to recognize the plural version of words that end in 'y' and are able to revert them to singular. For example, we can see words like properties and tendencies are reverted to property and tendency. Unfortunately, words that end in 's' but are not plural are also stemmed, resulting in words that may not be in the dictionary. For example, words like 'has' and 'less' return 'ha' and 'le'. Besides reverting plural to singular, the lemmatizer doesn't appear to recognize other endings like 'er', 'ing', and 'tion'."
      ],
      "metadata": {
        "id": "MTYe4GTR_PKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dist = nltk.FreqDist(lem_words)"
      ],
      "metadata": {
        "id": "E9Ie9m-lkBXp"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, to get the counts of each word, we can use the `FreqDist` function from the `nltk` library. This function takes words as an input and returns a dictionary with the unique words as keys and their respective frequencies as values. Let's observe the results of this function starting with the most frequent words first."
      ],
      "metadata": {
        "id": "FdpVE3liuBTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = [z for z in filter(lambda y:y[1]>=5, sorted(dict(dist).items(), key=lambda x:x[1], reverse=True))]\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqYVCe3ykm40",
        "outputId": "7fb2ddc1-9901-498b-e2a2-6a11af494973"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('a', 47),\n",
              " ('the', 44),\n",
              " ('of', 34),\n",
              " ('and', 24),\n",
              " ('we', 20),\n",
              " ('in', 19),\n",
              " ('to', 18),\n",
              " ('number', 16),\n",
              " ('internet', 15),\n",
              " ('different', 12),\n",
              " ('that', 12),\n",
              " ('as', 12),\n",
              " ('topology', 11),\n",
              " ('type', 11),\n",
              " ('network', 10),\n",
              " ('with', 9),\n",
              " ('on', 9),\n",
              " ('is', 8),\n",
              " ('an', 7),\n",
              " ('for', 7),\n",
              " ('our', 7),\n",
              " ('level', 6),\n",
              " ('taxonomy', 6),\n",
              " ('other', 6),\n",
              " ('from', 6),\n",
              " ('section', 6),\n",
              " ('this', 5),\n",
              " ('evolution', 5)]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To no surprise, the seven most frequent words present are words that can be considered as 'stop-words'. Aside from stop-words, we can see words like 'number', 'internet', 'different', 'topology', 'type' and 'network' appear at least 10 times. Based on the context of these words, we can gage that the text has to do with a topic relating to technology or mathematics. Other frequent words such as 'taxonomy' and 'evolution' suggest the text definitely discusses the sciences."
      ],
      "metadata": {
        "id": "ARxj96mfxNh4"
      }
    }
  ]
}